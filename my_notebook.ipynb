{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gtc-training-dli-logo-407-u.png\" alt=\"DLI-logo\" width=\"450\" height=\"450\" align=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Reinforcement Learning Agents to Play Starcraft II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the lab! Before we get started here are a few pointers on jupyter notebooks.\n",
    "\n",
    "1. This Jupyter Notebook is being rendered on your browser, but the contents are being streamed by an interactive iPython kernel running on an AWS EC2 GPU enabled instance.\n",
    "\n",
    "2. The notebook is composed of cells; cells can contain code which you can run, or they can hold text and/or images which are there for you to read.\n",
    "\n",
    "3. You can execute code cells by clicking the ```Run``` icon in the menu, or via the following keyboard shortcuts ```Shift-Enter``` (run and advance) or ```Ctrl-Enter``` (run and stay in the current cell).\n",
    "\n",
    "4. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a markdown cell -- if you run me I'll turn into static text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1  # i'm a code cell -- if you run me I'll perform some computations and show you their result below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a sub-field of Machine Learning in which the learning agent does not start with a labeled dataset of examples, but instead it builds its own dataset of experiences by interacting with the environment. Initially the agent's action are random, but as the agent stumbles onto good behaviors it is rewarded by the environment and this signal allows the agent to update its parameters so as to maximize the collection of future rewards.\n",
    "\n",
    "<img src=\"images/RL.jpg\" width=\"95%\"></img>\n",
    "\n",
    "RL has recently gained a lot of popularity since Google's DeepMind used RL and deep learning (DL) to train AI agents that can master Atari games by learning how to play the game from pixels alone.  Since then, DeepMind has used deep reinforcement learning (DRL) to beat Go.\n",
    "\n",
    "The next great unsolved challenge in RL is StarCraft II -- <a href=\"https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/\">read more</a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarCraft II\n",
    "StarCraft II (SC2) is a real-time strategy game developed by Blizzard that is free to play (<a href='https://starcraft2.com/en-us/'>link to official game site</a>). \n",
    "Click the image to watch an overview video.\n",
    "\n",
    "<a href='https://www.youtube.com/watch?v=yaqeZ9Snt4E'> <img src=\"images/sc2.jpg\"></img> </a>\n",
    "    \n",
    "Real-Time Strategy games require many skills including: strategic thinking, accurate/fast execution, information gathering/hiding, and economic resource management. In this lab we'll be training neural network agents to playing mini-games which isolate and capture some of the basic skills of the full game. \n",
    "\n",
    "<a href=\"\"><img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/mini-games.gif\" width=\"90%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is composed of the following sections:\n",
    "\n",
    "    \n",
    "<ul>\n",
    "    <a href='#section1'>Section#1</a> Programmatically explore and control SC2 using DeepMind's Pysc2 API\n",
    "    <br>&nbsp;<br>\n",
    "    <a href='#section2'>Section#2</a> Learn about how rewards can be used to shape agent behavior\n",
    "    <br>&nbsp;<br>\n",
    "    <a href='#section3'>Section#3</a> Tracking the Learning Process\n",
    "    <br>&nbsp;<br>\n",
    "    <a href='#section4'>Section#4</a> [ Mix & Match ] Deploy a trained agent in its target map or a new/foreign setting \n",
    "    <br>&nbsp;<br>\n",
    "    <a href='#section5'>Section#5</a> Build/Modify your own deep learning SC2 agent in Keras (TensorFlow backend)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings(\"ignore\") # supress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Section1 -- The SC2 PyGame Client and the Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working collaboratively with Blizzard, DeepMind have released a high level Python API ( <a href=\"https://github.com/deepmind/pysc2\">pysc2</a> ) which we can use to build machine learning agents that interact with the StarCraft II engine. \n",
    "\n",
    "Let's load this library into our notebook with the following import command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try using pysc2 to launch a mini-game using the py-game rendering client which produces a low-resolution view of the game (this view is what our deep learning agent will be using to play).\n",
    "\n",
    "The mini-game we'll be loading up will allow us to use the mouse and keyboard to control a squad of 9 Terran Space <a href=\"http://us.battle.net/sc2/en/game/unit/marine\">Marines</a> squaring up against a set of 4 Zerg <a href=\"http://us.battle.net/sc2/en/game/unit/roach\">Roaches</a>.\n",
    "\n",
    "By running the command below you'll launch the sc2 py-game client loaded with the \"DefeatRoaches\" map. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-471f3d563737>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-471f3d563737>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m 'pysc2/bin/play.py' --map DefeatRoaches --max_game_steps 2000 > /dev/null 2>&1\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pysc2/bin/play --map DefeatRoaches --max_game_steps 2000 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and play on this map we will need to open another tab which will be running a remote graphical desktop connection with the instance hosting the lab in the AWS cloud.\n",
    "\n",
    "\n",
    "### <center>Click on [noVNC Server](http://dli-9624cfae103e-courses-nvidia-com-user-32173-e0e69a.westus2.cloudapp.azure.com:6900/?password=vncpassword) to see the Pysc2 client. </center>\n",
    "\n",
    "Next let's launch the game client which should show up in the VNC viewer. Note that after 600 game steps the mini-game will close and you'll be able to restart it (by re-running the cell or continue with the rest of the content). NOTE: Use Internet Explorer or Microsoft Edge as you may not be able to interact with the game in other browsers.\n",
    "\n",
    "You should now be able to select and interact with the game in the GUI that is displayed within the VNC viewer. Click and drag to select (the green marine circles) and right click on the spot you would like the selected units to move to.\n",
    "\n",
    "\n",
    "<img src=\"images/marine_vs_roach.jpg\" width=\"50%\"></img><img src=\"images/marine_vs_roach_in_game.jpg\" width=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious you can try other mini-games by replacing DefeatRoaches in the cell above with one of the other map names. Here are some mini-games you can launch.\n",
    "\n",
    "* CollectMineralShards\n",
    "* DefeatZerglingsAndBanelings\n",
    "* FindAndDefeatZerglings\n",
    "* MoveToBeacon\n",
    "\n",
    "Each minigame attempts to teach the AI a different skill needed to master SC2.  What skills must the agent learn to master DefeatRoaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> FAQ:</b> Some browsers do not allow for interactive play within the VNC session -- try switching browsers or disabling extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 -- Exploring Environment Observations Programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll learn how to create and interact with SC2 environments programmatically. Instead of directly using the mouse and keyboard we'll use the functional API to explore the observations produced by the SC2 py-game client as well as to issue commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sc2_RL_environment.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['--'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "\n",
    "# define environment flags\n",
    "env_args = dict(\n",
    "        map_name='DefeatRoaches',\n",
    "        step_mul=1, # How many time steps occur between each action-decision. A step_mul of 1 means an agent can choose one action per frame.\n",
    "        game_steps_per_episode=0, # no limit- but each map has a built-in max number of steps and will terminate after reaching that.\n",
    "        screen_size_px = ( 64, 64 ), \n",
    "        minimap_size_px = ( 32, 32 ),\n",
    "        visualize = True,\n",
    "        score_index = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn the environment -- may take a minute to launch\n",
    "env = sc2_env.SC2Env(**env_args) # ** syntax implies variable number named arguments\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Pysc2 can be used from Linux, Windows, or MacOS.  We'll be using Linux instances for this lab but the replays from agents can be downloaded and then viewed on your Windows or Mac machine later.  \n",
    "\n",
    "We can now inspect a sample raw observation which the sc2 Linux simulator makes available after each game step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=0, discount=1.0, observation={'cargo': array([], shape=(0, 7), dtype=int32), 'minimap': array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=int32), 'game_loop': array([1], dtype=int32), 'available_actions': array([  0,   1,   2,   3,   4,   5,   7,  12, 331, 332, 333, 334,  13,\n",
       "       274, 451, 452, 453], dtype=int32), 'screen': array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=int32), 'control_groups': array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]], dtype=int32), 'multi_select': array([[48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0],\n",
       "       [48,  1, 45,  0,  0,  0,  0]], dtype=int32), 'cargo_slots_available': array([0], dtype=int32), 'player': array([1, 0, 0, 9, 0, 9, 0, 0, 9, 0, 0], dtype=int32), 'single_select': array([[0, 0, 0, 0, 0, 0, 0]], dtype=int32), 'build_queue': array([], shape=(0, 7), dtype=int32), 'score_cumulative': array([  0,   0,   0, 450,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of interesting data elements in the observation. If you look carefully you should be able to find the reward information for the current timestep, the cumulative score, as well as many other details about the state of the world including the available actions. Since we've just entered a new game world many of these values do not have a particularly informative state, however we'll come back to them again a bit later once we've taken some actions. Here we just demonstrate how their values can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('current reward: ', 0)\n",
      "('cumulative score: ', 0)\n",
      "('available data elements: ', ['cargo', 'minimap', 'game_loop', 'available_actions', 'screen', 'control_groups', 'multi_select', 'cargo_slots_available', 'player', 'single_select', 'build_queue', 'score_cumulative'])\n"
     ]
    }
   ],
   "source": [
    "print( 'current reward: ', obs[0].reward )\n",
    "print( 'cumulative score: ', obs[0].observation['score_cumulative'][0])\n",
    "print( 'available data elements: ', obs[0].observation.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specifically look at just the screen's visual layers. \n",
    "\n",
    "These visual \"feature layers\" represent elements of the game such as unit type, unit owner (player 1 vs player 2), unit health and other important aspects of the game state. Each feature layer is provided as a separate input to the network but since they all share the same spatial reference frame, they can be thought of as different channels/dimensions that come together to form a complete representation of the game state (i.e., similar to how the Red, Green, and Blue color channels are represented in photographs)\n",
    "\n",
    "<img src=\"images/feature_layers.png\" style=\"height:350px\">\n",
    "<center>\n",
    "**Note, this is a visualization of a very active screen in the middle of a late-game battle. We won't be seeing such a complex scene while playing mini-games.*\n",
    "</center>\n",
    "\n",
    "\n",
    "First we'll visualize all the data channels and their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 17 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numScreenLayers = obs[0].observation['screen'].shape[0] # 17\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "titles = ['heigh_map', 'visibility_map', 'creep', 'power', 'player_id', \n",
    "          'player_relative', 'unit_type', 'selected', \n",
    "          'unit_hit_points', 'unit_hit_points_ratio', \n",
    "          'unit_energy', 'unit_energy_ratio', \n",
    "          'unit_shields', 'unit_shields_ratio', \n",
    "          'unit_density', 'unit_density_aa', 'effects']\n",
    "[ [plt.subplot(5, 4, iScreenLayer+1), plt.imshow( obs[0].observation['screen'][iScreenLayer], aspect='equal'), \n",
    "       plt.title(titles[iScreenLayer]), plt.axis('off')] \n",
    " for iScreenLayer in range(numScreenLayers) ]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can zoom in on a single of these, to see it in more detail. Try changing the screen index and re-running the cell below. A description of what all the layers mean can be found <a href='https://github.com/deepmind/pysc2/blob/master/docs/environment.md#feature-layers'>at the following link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "screenIndex = 14 # choose a number between 0 and 16\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow( obs[0].observation['screen'][screenIndex])\n",
    "plt.title(titles[screenIndex])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 -- Taking Actions with the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've explored the observations produced by the sc2 engine. Let's try to take some actions that we can send to the engine so that it can respond and generate the next step/observation. \n",
    "\n",
    "The python functional API for taking actions in StarCraft enables us to produce the entire set of actions available to human players interacting through the mouse and keyboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what actions are currently available to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/no_op ()\n",
      "1/move_camera (1/minimap [0, 0])\n",
      "2/select_point (6/select_point_act [4]; 0/screen [0, 0])\n",
      "3/select_rect (7/select_add [2]; 0/screen [0, 0]; 2/screen2 [0, 0])\n",
      "4/select_control_group (4/control_group_act [5]; 5/control_group_id [10])\n",
      "5/select_unit (8/select_unit_act [4]; 9/select_unit_id [500])\n",
      "7/select_army (7/select_add [2])\n",
      "12/Attack_screen (3/queued [2]; 0/screen [0, 0])\n",
      "331/Move_screen (3/queued [2]; 0/screen [0, 0])\n",
      "332/Move_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "333/Patrol_screen (3/queued [2]; 0/screen [0, 0])\n",
      "334/Patrol_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "13/Attack_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "274/HoldPosition_quick (3/queued [2])\n",
      "451/Smart_screen (3/queued [2]; 0/screen [0, 0])\n",
      "452/Smart_minimap (3/queued [2]; 1/minimap [0, 0])\n",
      "453/Stop_quick (3/queued [2])\n"
     ]
    }
   ],
   "source": [
    "for iAction in obs[0].observation['available_actions']:\n",
    "    print( actions.FUNCTIONS[iAction] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how we can translate between a sequence of mouse and keyboard actions to functional API calls the following animated graphic is helpful.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/Oriol-Fig-Anim-170809-Optimised-r03.gif\"></img>\n",
    "\n",
    "Below are several example actions that we can issue using the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing\n",
    "doNothing = actions.FunctionCall( 0, [] ) \n",
    "\n",
    "# rectangle select and add to existing selection, rectangle from (0,0) to (31, 31)\n",
    "selectRectangle = actions.FunctionCall(3, [[1], [0,0], [31,31]])\n",
    "\n",
    "# select entire army\n",
    "selectEntireArmy = actions.FunctionCall(7, [[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's select all of our available army units and make them attack a point on the screen (and lets re-issue this order for the next 100 game steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack screen location ( x=60, y=15 ) -- assumes a mini-game with at least 64x64 tiles\n",
    "attackScreen = actions.FunctionCall(12, [[0], [60, 15]])\n",
    "\n",
    "obs = env.step( [ selectEntireArmy ] )\n",
    "for i in range(100):\n",
    "    obs = env.step( [ attackScreen ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 -- [Optional] Action Types & Build Your Own Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to provide additional detail about programmatic actions in SC2. Each action has a specific structure depending on whether it requires a modifier and/or spatial arguments. \n",
    "\n",
    "Below are several common possible action signatures (not an exhaustive list):\n",
    "\n",
    "```\n",
    "Type1: action.FunctionCall( functionID )\n",
    "Type2: action.FunctionCall( functionID, [ [ modifier ] ] ) # e.g., 'select_army'\n",
    "Type3: action.FunctionCall( functionID, [ [ modifier ], [x1, y1 ] ) # e.g., 'attack_screen'\n",
    "Type4: action.FunctionCall( functionID, [ [ modifier ], [x1, y1], [x2, y2] ) # e.g., 'select_rect'\n",
    "```\n",
    "<b>Type1</b> are simple actions such as 'Stop_quick' (ID: 453) or 'HoldPosition_quick' (ID: 274) which do not require any modifiers or screen coordinates. \n",
    "\n",
    "<b>Type2</b> actions require a modifier -- in the case of 'select_army' (ID: 7) [which selects all living army units] the modifier  indicates whether the returned army unit set should be added to whatever is currently selected (e.g., a worker unit) or replace the existing selection. \n",
    "\n",
    "<b>Type3</b> actions require a modifier and a single coordinate -- e.g., 'Attack_screen' (ID: 12), has a modifier that indicates whether the action should be done immediately (or added to a queue for execution) as well as a single coordinate target.\n",
    "\n",
    "<b>Type4</b> actions require a modifier and two screen coordinates -- e.g., 'select_rect' (ID: 3) which require both a modifier (whether to replace, concatenate, or subtract the new selection from the existing selection) as well as two screen coordinates indicating the bounding box that defines the new selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RL agents can only interact with the game world using this programmatic interface -- can you try to create an interesting sequence of actions below?\n",
    "\n",
    "<b>Helpful Hints:</b> If you need to, feel free to <b>reset</b> the game state to the beginning with the following command ```obs = env.reset()```\n",
    "We've also written a ```safe_action``` function (a few cells below) that you can your can use to help avoid actions that are not permitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "obs = env.reset()\n",
    "obs = env.step( [ selectEntireArmy ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 -- Random Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we've written the code for an agent that randomly chooses points to attack. You may notice that even the random agent can stumble onto good behaviors and in fact this is what we rely on to bootstrap the learning process when we initially randomly initialize the parameters of our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the action we chose can't be executed lets try to select our entire army, \n",
    "advance the game state, and try again.\n",
    "\"\"\"\n",
    "\n",
    "def safe_action ( actionToTake, obs ):\n",
    "    if actionToTake.function not in obs.observation['available_actions']:\n",
    "        print('unable to take selected action...lets try to fix things')\n",
    "        print('fix#1: select all army units')\n",
    "        obs = env.step( [ selectEntireArmy ] )\n",
    "        print('fix#2: perform no-op action')\n",
    "        obs = env.step( [ doNothing ] )\n",
    "        if actionToTake.function not in obs[0].observation['available_actions']:\n",
    "            print('!we are really in trouble...consider taking a different action')\n",
    "    else:\n",
    "        obs = env.step( [ actionToTake ] )\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to take selected action...lets try to fix things\n",
      "fix#1: select all army units\n",
      "fix#2: perform no-op action\n",
      "unable to take selected action...lets try to fix things\n",
      "fix#1: select all army units\n",
      "fix#2: perform no-op action\n"
     ]
    }
   ],
   "source": [
    "# obs = env.reset()\n",
    "nCycles = 1000\n",
    "for iCycles in range ( nCycles ):    \n",
    "    randomAttackScreen = actions.FunctionCall( 12, [[0], [np.random.randint(63), np.random.randint(63)]])\n",
    "    obs = safe_action ( randomAttackScreen, obs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Section 2 -- Reward Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents trained by DRL can learn behaviors depending on the environment (simulator) that they train in and also the rewards that the environment gives them.  We trained two agents in the same environment, a simple minimap with two roach packs split apart.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 -- Specifying Rewards Through the SC2 Map Editor\n",
    "We used the map-editor to change the logic which governs the reward generated by the environment. If you'd like to build your own mini-games or modify existing ones, download and install the free to play Starcraft client which includes the map-editor. You can then edit maps by modifying the terrain and/or adjusting the logic in the triggers. Below is a screenshot from the logic of the default DefeatRoaches map which increases the score by 10 every time a roach is killed (just for reference: player one is our RL agent and player two is the in-game scripted AI).\n",
    "\n",
    "<img src=\"images/map_editor.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 -- Reward Modifications and Emergent Behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SC2 Map Editor we built several variations of the DefeatRoaches Map. In one case we encouraged the agent to be aggressive by giving a penalty for every elapsed second. Execute the cell below to see a recording of this agent after it was trained for an extended duration. Notice that the above agent is always on the move and is always looking for enemies to kill.  The agent can get a higher score by finding and killing all of the roaches more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8eRFzXtBdwA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8eRFzXtBdwA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we built a map that more heavily penalized the loss of squad members. Execute the cell below to see this conservative agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Opgktl6kLo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9Opgktl6kLo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the aggressive agent is always on the move and looking for roaches to kill, the conservative agent is more than happy to hang out in the corner and not lose any marines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Section 3 -- Tracking the Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand a lot about our agents by looking at the rewards that they are able to earn over time.  By plotting these curves with respect to time training or number of frames seen, we get a reward curve.  We've plotted our curves using Tensorboard from Tensorflow.  You can inspect the curves for a number of different agents at this [link](/tensorboard/).\n",
    "\n",
    "Click \"toggle all runs\" then click on the run you want to explore.  Many metrics are captured sc2/episode_score has the score that the agent was given from the environment.\n",
    "\n",
    "The agents were trained using this open source repo: https://github.com/simonmeister/pysc2-rl-agents.git Here's an image of a reward curve for an agent training in the DefeatRoaches minigame.\n",
    "<center>\n",
    "<img src=\"images/DefeatRoaches.PNG\" width=\"75%\"></img>\n",
    "</center>\n",
    "As you can see from the learning curve, often times the agent has to sacrifice its score in order to learn a new strategy that will ultimately lead to better play.  Here are two videos.  The first is the agent half-way through its training and the second is the agent when fully trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 -- Half-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tzPrtTXPTEA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tzPrtTXPTEA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the agent has learned to focus down the roaches some.  However, it's still learning and spends some time clicking around the map. \n",
    "\n",
    "By the end of the training, the agent is very efficient and focuses down the roaches with ease.  With some luck, the agent can score very high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IBUgp6097Q0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IBUgp6097Q0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 -- Multiple Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having your agent play in multiple environments simultaneously, the agent can learn much faster.  We trained one agent using 16 environments simultaneously and another agent using only one environment.\n",
    "\n",
    "\n",
    "<img src=\"images/MoveToBeaconNenvs.PNG\" width=\"75%\"></img>\n",
    "The agent with 16 environments was able to master the MoveToBeacon minimap in 20 minutes while the agent with 1 environment took 2 hours and 20 minutes.\n",
    "\n",
    "MoveToBeacon is a simple minigame and so an agent learning in only one environment can still master the game.  For more complex environments, agents often need to train in multiple environments in order to learn the advanced strategies required to master the game.\n",
    "\n",
    "Browse [TensorBoard](/tensorboard/) in order to explore the reward curves more.  TensorBoard also has the reward curve for an agent that trained on 8 envs simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# Section 4 -- Mix and Match Agents and Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you can choose an agent and then have that agent play in a mini-game.  You're free to experiment and see how different agents do in different environments. Below is a list of the available agents and the environments you can deploy them to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Agents \n",
    "\n",
    "* <b>DefeatRoaches</b> -- fully trained agent on the DefeatRoaches map\n",
    "* <b>DefeatRoaches_half_trained</b> -- partially trained agent on the DefeatRoaches map\n",
    "* <b>DefeatRoaches_singleRoundReset_conserveMarines_noTimePenalty_splitRoachPacks</b> -- agent tries hard not to lose units\n",
    "* <b>DefeatRoaches_singleRoundReset_highTimePenalty_splitRoachPacks</b> -- agent has to hurry to kill opponent\n",
    "* <b>MoveToBeacon_n_envs_16</b> -- agent trained to move to a target beacon\n",
    "\n",
    "### Environments [ <a href=\"https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md\">detailed descriptions</a> ]\n",
    "<ul>\n",
    "BuildMarines <b>||</b> CollectMineralShards <b>||</b> CollectMineralsAndGas <br >\n",
    "DefeatRoaches <b>||</b> DefeatZerglingsAndBanelings <b>||</b> FindAndDefeatZerglings <br >\n",
    "MoveToBeacon\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load an agent, we have to specify the map and the checkpoint. Put the agent's name below to see the model checkpoint number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\t       model.ckpt-250000.index\r\n",
      "model.ckpt-250000.data-00000-of-00001  model.ckpt-250000.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls /notebooks/models/DefeatRoaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"checkpoint\" file points to the model to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"model_checkpoint_path: \\\"/notebooks/models/DefeatRoaches/model.ckpt-250000\\\"\" > /notebooks/models/DefeatRoaches/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replay will be written to disk after 10 episdoes have been played.\n",
    "\n",
    "When you are finished click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu, and select ```Interrupt ``` so that you can execute more cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2019-03-20 21:28:59.241263: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Version: B55958 (SC2.3.16)\n",
      "Build: Jul 31 2017 13:19:41\n",
      "Command Line: '\"/headless/StarCraftII/Versions/Base55958/SC2_x64\" -listen 127.0.0.1 -port 21440 -dataDir /headless/StarCraftII/ -tempDir /tmp/sc-kz9wS_/ -displayMode 0'\n",
      "Starting up...\n",
      "Startup Phase 1 complete\n",
      "2019-03-20 21:28:59.358961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 3433:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2019-03-20 21:28:59.359031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 3433:00:00.0, compute capability: 3.7)\n",
      "Startup Phase 2 complete\n",
      "Creating stub renderer...\n",
      "Listening on: 127.0.0.1:21440 (21440)\n",
      "Startup Phase 3 complete. Ready for commands.\n",
      "Requesting to join a single player game\n",
      "Configuring interface options\n",
      "Configure: raw interface enabled\n",
      "Configure: feature layer interface enabled\n",
      "Configure: score interface enabled\n",
      "Configure: render interface disabled\n",
      "Entering load game phase.\n",
      "Launching next game.\n",
      "Next launch phase started: 2\n",
      "Next launch phase started: 3\n",
      "Next launch phase started: 4\n",
      "Next launch phase started: 5\n",
      "Next launch phase started: 6\n",
      "Next launch phase started: 7\n",
      "Next launch phase started: 8\n",
      "Game has started.\n",
      "Sending ResponseJoinGame\n",
      "ALSA lib confmisc.c:768:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "Loaded agent at train_step 250000\n",
      "episode 0: score = 71.000000\n",
      "Game has started.\n",
      "episode 1: score = 21.000000\n",
      "Game has started.\n",
      "episode 2: score = 96.000000\n",
      "Game has started.\n",
      "episode 3: score = 71.000000\n",
      "Game has started.\n",
      "episode 4: score = 56.000000\n",
      "Game has started.\n",
      "episode 5: score = 71.000000\n",
      "Game has started.\n",
      "episode 6: score = 46.000000\n",
      "Game has started.\n",
      "episode 7: score = 91.000000\n",
      "Game has started.\n",
      "episode 8: score = 36.000000\n",
      "Game has started.\n",
      "episode 9: score = 262.000000\n",
      "Game has started.\n",
      "episode 10: score = 81.000000\n",
      "Game has started.\n",
      "episode 11: score = 36.000000\n",
      "Game has started.\n",
      "episode 12: score = 46.000000\n",
      "Game has started.\n",
      "episode 13: score = 46.000000\n",
      "Game has started.\n",
      "episode 14: score = 56.000000\n",
      "Game has started.\n",
      "episode 15: score = 46.000000\n",
      "Game has started.\n",
      "episode 16: score = 125.000000\n",
      "Game has started.\n",
      "episode 17: score = 243.000000\n",
      "Game has started.\n",
      "episode 18: score = 21.000000\n",
      "Game has started.\n",
      "episode 19: score = 71.000000\n",
      "Game has started.\n",
      "episode 20: score = 46.000000\n",
      "Game has started.\n",
      "episode 21: score = 46.000000\n",
      "Game has started.\n",
      "episode 22: score = 56.000000\n",
      "Game has started.\n",
      "episode 23: score = 46.000000\n",
      "Game has started.\n",
      "episode 24: score = 81.000000\n",
      "Game has started.\n",
      "episode 25: score = 81.000000\n",
      "Game has started.\n",
      "episode 26: score = 56.000000\n",
      "Game has started.\n",
      "episode 27: score = 106.000000\n",
      "Game has started.\n",
      "episode 28: score = 46.000000\n",
      "Game has started.\n",
      "episode 29: score = 46.000000\n",
      "Game has started.\n",
      "episode 30: score = 36.000000\n",
      "Game has started.\n",
      "episode 31: score = 56.000000\n",
      "Game has started.\n",
      "episode 32: score = 21.000000\n",
      "Game has started.\n",
      "episode 33: score = 46.000000\n",
      "Game has started.\n",
      "W0320 21:30:08.794471 139675241711360 sc_process.py:183] Killing the process.\n",
      "mean score: 69.441176\n"
     ]
    }
   ],
   "source": [
    "!python /notebooks/pysc2-rl-agents/run.py \\\n",
    "  DefeatRoaches \\\n",
    "  --map DefeatRoaches \\\n",
    "  --max_windows 1 --gpu 0 --envs 1 \\\n",
    "  --step_mul 8 --steps_per_batch 16 \\\n",
    "  --vis --eval \\\n",
    "  --save_dir /notebooks/models \\\n",
    "  --summary_dir /notebooks/summary \\\n",
    "  --iters 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done generating replays, you can execute the cell below to zip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: notebooks/replays/DefeatRoaches_2019-03-20-21-29-27.SC2Replay (deflated 12%)\r\n",
      "  adding: notebooks/replays/DefeatRoaches_2019-03-20-21-29-44.SC2Replay (deflated 8%)\r\n",
      "  adding: notebooks/replays/DefeatRoaches_2019-03-20-21-30-00.SC2Replay (deflated 6%)\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get update > /dev/null 2>&1 && apt-get install zip > /dev/null 2>&1 && zip /notebooks/replays.zip /notebooks/replays/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to download your replays.](replays.zip)\n",
    "\n",
    "To view your replays using your local client, put the replay file in ```~/StarCraftII/replays``` and and the minigames to ```~/StarCraftII/Maps/mini_games```.  You can download the minigames from [here](https://github.com/deepmind/pysc2/tree/master/pysc2/maps/mini_games)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# Section 5 -- Build and Train your Own Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you've enjoyed the lab so far and hopefully learned something new about RL & SC2. We've only barely scratched the surface of what's possible and if you'd like to go further we hope to offer some tools for additional exploration.\n",
    "\n",
    "Below we have built a training harness for a custom agent using Keras with a TensorFlow back-end. Our goal is to offer an approachable sandbox for anyone interested in experiment with RL ideas in the SC2 context.\n",
    "\n",
    "This code harness is structured as follows:\n",
    "<ul>\n",
    "1 -- Define Key Parameters <br>\n",
    "2 -- Create Multiple SC2 Environments <br>\n",
    "3 -- Load the Agent Code <br>\n",
    "4 -- Run the Interaction Loop (using trajectories of n-steps) and Update Agent Parameters <br>\n",
    "</ul>\n",
    "For the technically inclined, the training harness uses n-step TD learning and can stack together sequences of observation frames in order to learn about dynamics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require([\"base/js/dialog\"], function(dialog) {dialog.modal({title: 'Cleaning Up', body: 'We need to ensure that no other sc2 environments are active -- lets force a kernel restart', buttons: {'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }}});});\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os.path\n",
    "\n",
    "# required \n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['initialize FLAGS for sc2 environments'])\n",
    "\n",
    "# note this import must happen after flag initialization\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 - Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envParams = { \n",
    "    'simultaneousEnvironments': 4,\n",
    "    'nEnvironmentsToVisualize': 4,\n",
    "    'nTrajectorySteps': 16,\n",
    "    'nStackedFrames': 4,\n",
    "    'agentStepsPerEnvironmentStep': 4,\n",
    "    \n",
    "    'batchCheckpointInterval': 100,\n",
    "    'experimentDirectory': '/experiments',\n",
    "    'debugFlag': False,\n",
    "    \n",
    "    'screenResX': 64,\n",
    "    'screenResY': 64,\n",
    "    'screenResX_minimap': 32,\n",
    "    'screenResY_minimap': 32,\n",
    "    \n",
    "    'screenChannelsToKeep': [ 4, 5, 6, 7, 8, 9 ], # player_ID, player_relative, unit_type, selected, unit_hit_points, unit_hit_points_ratio\n",
    "    'screenChannelsRetained': 6,\n",
    "\n",
    "    'nonSpatialInputDimensions': 12, \n",
    "    'allowedActionIDs': [ 3, 12 ], # select_rect, attack_screen\n",
    "    'allowedActionIDRequiresModifier': [ 2, 1 ],            \n",
    "    'allowedActionIDRequiresLocation': [ 2, 1 ],       \n",
    "\n",
    "    'prunedActionSpaceSize': 2,\n",
    "    'actionArgumentSize': 4, \n",
    "\n",
    "    'nonVisualInputLength': 13,\n",
    "    \n",
    "    'futureDiscountRate': 1,   \n",
    "    'stepTypeFirst': 0,\n",
    "    'stepTypeMid': 1,\n",
    "    'stepTypeLast': 2,\n",
    "    \n",
    "    'entropyWeight': .25,\n",
    "    'policyWeight': 1,\n",
    "    'valueWeight': .5,\n",
    "    \n",
    "}\n",
    "\n",
    "# sanity check environment parameter definition\n",
    "assert ( envParams['prunedActionSpaceSize'] == len(envParams['allowedActionIDs']) \\\n",
    "            == len(envParams['allowedActionIDRequiresModifier']) \\\n",
    "            == len(envParams['allowedActionIDRequiresLocation']) )\n",
    "assert ( envParams['screenChannelsRetained'] == len(envParams['screenChannelsToKeep'] ) )\n",
    "\n",
    "\n",
    "assert ( envParams['nStackedFrames'] <= envParams['nTrajectorySteps'])\n",
    "\n",
    "sc2EnvLaunchParams = {\n",
    "    'map_name':'DefeatRoaches',\n",
    "    'step_mul': envParams['agentStepsPerEnvironmentStep'],\n",
    "    'game_steps_per_episode': 0, # no limit\n",
    "    'screen_size_px': ( envParams['screenResX'], envParams['screenResX']), \n",
    "    'minimap_size_px': ( envParams['screenResX_minimap'], envParams['screenResY_minimap']),\n",
    "    'visualize': False,\n",
    "    'score_index': None \n",
    "}\n",
    "sc2EnvLaunchParamsVis = sc2EnvLaunchParams.copy()\n",
    "sc2EnvLaunchParamsVis['visualize'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 -- Launch Multiple Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll fork multiple parallel worker processes (each with its own memory space). Each of which will be running an instance of the sc2 linux client. Each worker process will remain connected to the current process (the notebook kernel) via a two-way communication pipe through which we'll send requests and interact with the sc2 environment inside it.\n",
    "\n",
    "<img src=\"images/multi_process.PNG\" style=\"width:40%\">\n",
    "\n",
    "For each pipe, the end attached to our main process is known as the local end, and the end attached to the worker is known as the remote end. We communicate between the processes using the pipe ends and by transmitting using ```.send()``` commands and listening for responses using ```.recv()``` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "# define the function running in each [forked] worker to parse communications between itself and the main process\n",
    "def sc2_remote_env_manager ( workerID, remotePipeEnd, sc2EnvLaunchParams ):\n",
    "    print('starting remote sc2 environment thread# ' + str(workerID))\n",
    "    env = sc2_env.SC2Env(**sc2EnvLaunchParams)\n",
    "    obs = env.reset()    \n",
    "    remotePipeEnd.send( ( 'launch complete', obs[0]) )\n",
    "    \n",
    "    while True:\n",
    "        command, arguments = remotePipeEnd.recv()\n",
    "        # take action and advance the game environment \n",
    "        if command == 'step': \n",
    "            obs = env.step( [ arguments ] )\n",
    "            assert( len(obs) == 1 )\n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        elif command == 'reset':\n",
    "            obs = env.reset()    \n",
    "            remotePipeEnd.send( obs[0] )\n",
    "        # close the pipe and sc2 environment\n",
    "        elif command == 'close':\n",
    "            remotePipeEnd.send('closing')\n",
    "            remotePipeEnd.close()\n",
    "            break\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "localPipeEnds = [] # pipe-ends/communication channels used by the main process to communicate with workers\n",
    "remotePipeEnds = [] # pipe-ends/communication channels used by the workers to communicate with the main process\n",
    "processList = [] # list of worker threads\n",
    "\n",
    "# create two-way-communication channels [aka pipes] for remote workers and our main program\n",
    "# and spawn remote processes with remote pipe ends as an input argument\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    # create new pipe\n",
    "    localPipeEnd, remotePipeEnd = Pipe()\n",
    "    # store both ends \n",
    "    localPipeEnds += [ localPipeEnd ]\n",
    "    remotePipeEnds += [ remotePipeEnd ]\n",
    "    # spawn remote process and connect to remote pipe end\n",
    "    if iEnv < envParams['nEnvironmentsToVisualize']:\n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParamsVis) ) ]\n",
    "    else:        \n",
    "        processList += [ Process( target = sc2_remote_env_manager , args = ( iEnv, remotePipeEnd, sc2EnvLaunchParams) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {}\n",
    "for iEnv in range ( envParams['simultaneousEnvironments'] ):\n",
    "    obs[iEnv] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start remote workers, wait for each process to fully bring up the sc2 environment before creating the next\n",
    "for iEnv in range(envParams['simultaneousEnvironments']):\n",
    "    processList[iEnv].start()\n",
    "    while not localPipeEnds[iEnv].poll():\n",
    "        time.sleep(1)\n",
    "    msg, obs[iEnv] = localPipeEnds[iEnv].recv()\n",
    "    print ( msg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Click on [noVNC Server](http://dli-9624cfae103e-courses-nvidia-com-user-32173-e0e69a.westus2.cloudapp.azure.com:6900/?password=vncpassword) to see the environments. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 -- Load Agent Code\n",
    "\n",
    "Here is [the code for a convolutional agent](../edit/convolutional_agent.py) -- feel free to play with it and make improvements!\n",
    "This agent inherits from the basic [sc2_agent](../edit/convolutional_agent.py) - you can think of this agent as the skeleton and the convolutional agent as the brain which does the actual feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' if using python3 uncomment -> ''' # from importlib import reload \n",
    "import convolutional_agent as sc2RL\n",
    "reload(sc2RL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sc2RL.ConvAgent( envParams )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environments\n",
    "for iEnv in range( envParams['simultaneousEnvironments'] ):\n",
    "    localPipeEnds[iEnv].send ( ( 'reset', [] ) )\n",
    "    obs[iEnv] = localPipeEnds[iEnv].recv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 -- Act - Observe - Learn [ Loop ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RL_diagram.PNG\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "trainBatches = 15 # change to a large number or replace first for loop with a 'while True:' loop\n",
    "\n",
    "for iTrajectory in range(trainBatches):\n",
    "    for iStep in range ( envParams['nTrajectorySteps'] + 1):        \n",
    "        ''' batch predict model outputs on current inputs ''' # [ 1 timestep in multiple environments ]\n",
    "        batchModelOutputs = agent.batch_predict ( np.squeeze( agent.nEnvOneStepBatch, axis=1),\n",
    "                                                      np.squeeze( agent.nEnvOneStepBatchNonSpatial, axis=1)  )\n",
    "        \n",
    "        ''' update trajectory rewards and value estimates '''\n",
    "        agent.rewards[:, iStep ] = [ obs[iEnv].reward for iEnv in list(obs.keys()) ]\n",
    "        agent.valuePredictions[:, iStep] = batchModelOutputs[:, agent.policyInds['value']]\n",
    "        \n",
    "        if iStep != envParams['nTrajectorySteps']: # don't advance when in the final step -- use it to bootstrap loss computation\n",
    "            ''' sample and mask '''\n",
    "            sc2functionCalls, actionIDs, actionArguments = \\\n",
    "                agent.sample_and_mask ( obs, batchModelOutputs )\n",
    "\n",
    "            ''' compute partial loss terms ''' # logProbs and masked policy entropy\n",
    "            agent.inplace_update_logProbs_and_entropy ( iStep, batchModelOutputs )\n",
    "\n",
    "            ''' step i.e. apply selected action in each environment ''' # and get new observations\n",
    "            obs, _ = agent.step_in_envs ( obs, localPipeEnds, sc2functionCalls, actionIDs )\n",
    "\n",
    "            ''' compile the spatial and non-spatial trajectory observations ''' # needed for batch train update\n",
    "            agent.inplace_update_trajectory_observations( iStep, obs )\n",
    "        \n",
    "    ''' finished generating a trajectory -- compute nStep returns, advantages, and cumulative loss '''\n",
    "    agent.compute_loss ()\n",
    "    agent.train()\n",
    "    \n",
    "    print( 'trajectory# ' + str(trainBatches) + ' avg step reward: ' + str(np.mean( np.mean( agent.rewards ))))\n",
    "    if (trainBatches + 1) % envParams['batchCheckpointInterval'] == 0:\n",
    "        agent.model_checkpoint()            \n",
    "    trainBatches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thank you for participating in the lab. If you'd like to explore the fascinating world of reinforcement learning further, feel free to explore the following resources:\n",
    "\n",
    "Udacity's deep learning reinforcement course\n",
    "https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893\n",
    "\n",
    "Deep mind's overview of deep learning reinforcement approaches\n",
    "https://deepmind.com/blog/deep-reinforcement-learning/\n",
    "\n",
    "Blog post discussing Policy Gradient Networks\n",
    "https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gtc-training-dli-logo-407-u.png\" alt=\"DLI-logo\" width=\"450\" height=\"450\" align=\"center\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
